Baseline SVM on dev set:

Classification Report:
              precision    recall  f1-score   support

         NOT       0.71      0.97      0.82       648
         OFF       0.82      0.28      0.42       352

    accuracy                           0.73      1000
   macro avg       0.77      0.62      0.62      1000
weighted avg       0.75      0.73      0.68      1000


Baseline using preprocessing of hashtag and emoji:

              precision    recall  f1-score   support

         NOT       0.72      0.97      0.82       648
         OFF       0.84      0.29      0.43       352

    accuracy                           0.73      1000
   macro avg       0.78      0.63      0.63      1000
weighted avg       0.76      0.73      0.69      1000



Classic model (SVM) notes:
vectorizer -> tftidf superior over count
ngram 1,2 (0.03 f1 less) or 1,3 (0.05 f1 less) -> scores worse while model takes longer to run
maxdf does nothing from values 1.0 until 0.7 after which scores just become worse
mindf 0.05->0.01 = bad, 0.0005 vert close to baseline dus lower = better (werkt dus niet)

lemmatising -> 0 difference

pos tags -> way worse scores (0.51 f1)


Papers from shared task that used SVM with scores on test set:
70% ish SVM - https://aclanthology.org/S19-2107.pdf
72% SVM - https://aclanthology.org/S19-2115.pdf